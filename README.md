# Mentor-KD: Making Small Language Models Better Multi-step Reasoners

**This repository will be renewed upon publication.**

This repository contains the code and datasets used for our study **"Mentor-KD: Making Small Language Models Better Multi-step Reasoners"** accepted to EMNLP 2024. In this paper, we improve reasoning distillation by leveraging task-specific intermediate-sized mentor models which complements insufficient distillation signals from the teacher model.

Details will appear soon!
